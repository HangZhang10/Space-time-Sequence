# 4 单维时间序列预测

第二种方法就是直接利用序列建模的分析方法，利用序列前面的窗口数据来预测序列后几个窗口的数据，一般无需构建时序衍生特征，比如典型的RNN、LSTM、GRU等，复杂点的模型一般会结合序列建模的常用方法，比如seq2seq、Attention等，这里有几个经典的模型比如DA_RNN、LSTnet等。
对于序列建模的方法，其重点比较多，这里主要介绍简单的序列建模方法。以天池城市计算大赛的数据为例，这里分别看看单维序列建模和多维序列建模：<br>

第一步，获取单维序列数据<br>

```
"""
时空序列数据探索：
第一步：单个站点的时间序列探索
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

total_train = pd.read_csv("../data/tianchi/Metro_train/testA_train.csv")


"""
单元时间序列
"""
#获取单元时间序列
single_train = total_train[['time','stationID','userID_in','userID_out']]
single_in = total_train[['time','stationID','userID_in']]
single_out = total_train[['time','stationID','userID_out']]

#获取第i个站点的数据
single_i_in = single_in[single_in['stationID']==0][['time','userID_in']]
#print(single_i_in.head())

plt.figure(figsize=(50, 10))
plt.plot(single_i_in['userID_in'])
plt.show()

#去掉第一天的数据
single_i_in.index = single_i_in['time']
single_i_in = single_i_in['2019-01-02':'2019-01-25']
#print(single_i_in.head())

#划分训练集和测试集,并删除掉站点关闭的时间（如果保留，用LSTM做预测效果很差）
for i in range(2,26):
    if i<10:
        start = '2019-01-0'+str(i)+' 05:59:59'
        end = '2019-01-0'+str(i)+' 23:59:59'
    else:
        start = '2019-01-'+str(i)+' 05:59:59'
        end = '2019-01-'+str(i)+' 23:59:59'
    if i<=18:
        if i == 2:
            single_itrain_in = single_i_in[start:end]
        else:
            single_itrain_in = pd.concat([single_itrain_in,single_i_in[start:end]])
    else:
        if i == 19:
            single_itest_in = single_i_in[start:end]
        else:
            single_itest_in = pd.concat([single_itest_in,single_i_in[start:end]])

#构造张量
single_itrain_in = np.array(single_itrain_in['userID_in'])
single_itest_in = np.array(single_itest_in['userID_in'])
print(single_itrain_in.shape)
print(single_itest_in.shape)

plt.plot(single_itest_in)
plt.show()

```

第二步，建立单维序列模型<br>

```
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error

class TsLSTM(nn.Module):
    def __init__(self, input_size, hidden_dim):
        super(TsLSTM,self).__init__()
        self.input_dim = input_size
        self.hidden_dim = hidden_dim
        self.lstm = nn.LSTM(input_size,hidden_dim,batch_first=True)
        self.hidden2out = nn.Linear(hidden_dim,1)
        
    def init_hidden(self,batch_size):
        return (Variable(torch.zeros(1, batch_size, self.hidden_dim)),
                Variable(torch.zeros(1, batch_size, self.hidden_dim)))
    
    def forward(self, seq, batch_size):
        self.hidden = self.init_hidden(batch_size)
        lstm_out, self.hidden = self.lstm(seq, self.hidden)
        out = self.hidden2out(lstm_out.contiguous().view(seq.shape[0]*seq.shape[1], -1))
        out = out.contiguous().view(seq.shape[0], seq.shape[1], -1)
        
        return out

#获取便于模型训练所需的训练/测试集
def make_data(train_or_test, k):
    x, y = [], []
    for i in range(len(train_or_test)-k-1):
        x.append(train_or_test[i:i+k])
        y.append(train_or_test[i+1:i+k+1])
    x, y = np.array(x), np.array(y)
    x = np.reshape(x, (x.shape[0], x.shape[1], 1))
    
    return x, y
    
#训练函数
def train_NN(model, x_train, y_train, args):
    optimizer = optim.RMSprop(model.parameters(), lr=args['lr'], momentum=0.9)
    criterion = nn.MSELoss()
    
    n = x_train.shape[0]
    print("x_train num is:", n)
    batchNum = n //args['batch_size'] - 1
    
    for epoch in range(args['epochs']):
        x_train, y_train = shuffle(x_train, y_train, random_state=epoch)

        batchStart = 0
        lossSum = 0

        for batch_idx in range(batchNum):
            x = x_train[batchStart:batchStart+args['batch_size'], :, :]
            y = y_train[batchStart:batchStart+args['batch_size']]
            
            optimizer.zero_grad()
            x = Variable(torch.FloatTensor(x))
            y = Variable(torch.FloatTensor(np.array(y)))
            out = model(x,args['batch_size'])
            loss = criterion(out, y)
            loss.backward(retain_graph=True)
            lossSum += loss.data.numpy()
            optimizer.step()
            
            batchStart += 1
        print("------第%s次迭代-----" % str(epoch))
        print("loss:%s" % str(lossSum*1.0/args['batch_size']))
        
    return model

#测试函数
def predict_NN(model, x_test, y_test, args):

    batchStart = 0
    
    n = x_test.shape[0]
    print("x_test num is:", n)
    batchNum = n //args['batch_size'] - 1
    
    for batch_idx in range(batchNum+1):
        x = x_test[batchStart:batchStart+args['batch_size'], :, :]
        x = Variable(torch.FloatTensor(x))
        out = model(x,args['batch_size'])
        out_tmp = out[:,-1,:].data.numpy()
        out_tmp = out_tmp.reshape(out_tmp.shape[0])
        if batch_idx == 0:
            pred = out_tmp
        else:
            pred = np.concatenate([pred,out_tmp])
    
    y_test = np.array(y_test)[:, -1, :]
    y_test = y_test.reshape(y_test.shape[0])[:300]
    pred = np.array(pred)
    
    RMSE = np.sqrt(mean_squared_error(pred, y_test, multioutput='uniform_average'))
    print(RMSE)
    
    return pred, y_test


#归一化
scaler = MinMaxScaler(feature_range=(0, 1))
single_itrain_in = single_itrain_in.reshape(-1, 1)
single_itrain_in = scaler.fit_transform(single_itrain_in)
single_itest_in = single_itest_in.reshape(-1, 1)
single_itest_in = scaler.transform(single_itest_in)

k = 300
x_train, y_train = make_data(single_itrain_in, k)
x_test, y_test = make_data(single_itest_in, k)

#建立模型
model = TsLSTM(1, 50)

#训练
args = {'lr':0.00005, 'epochs':150, 'batch_size':300}
model = train_NN(model, x_train, y_train, args)

#测试
pred, y_test = predict_NN(model, x_test, y_test, args)

#画图
plt.plot(y_test)
plt.plot(pred)
plt.show()
```

------第145次迭代-----<br>
loss:6.986272521317005e-05<br>
------第146次迭代-----<br>
loss:7.070056317994992e-05<br>
------第147次迭代-----<br>
loss:7.016174650440614e-05<br>
------第148次迭代-----<br>
loss:7.03214667737484e-05<br>
------第149次迭代-----<br>
loss:7.188943525155385e-05<br>
x_test num is: 347<br>
0.0689800541321792<br>

