# 8.STGCN预测人流量案例分析

还是以阿里天池：城市计算大赛的数据作为案例的数据集，大家可以前往下载：<br>
https://tianchi.aliyun.com/competition/entrance/231712/introduction?spm=5176.12281973.1005.7.3dd52448dQdGgJ<br>

数据集包括：<br>
- 各站点的时间序列数据<br>
- 各站点之间的网络图<br>

## 本文思路：
- 第一部分：构造数据集<br>
- 第二部分：构造STGCN模型<br>
- 第三部分：构造STGCN模型所需的数据<br>
- 第四部分部分：训练并评估模型<br>


第一部分：构造数据集<br>

```
total_train = pd.read_csv("../data/tianchi/Metro_train/testA_train.csv")

#去掉第一天的数据
total_train['time'] = pd.to_datetime(total_train['time'])
total_train.index = total_train['time']
total_train = total_train['2019-01-02':'2019-01-25']

#删除多余字段
total_train = total_train.drop('userID_out',axis=1)

#划分训练集和测试集
itrain_in = total_train['2019-01-02':'2019-01-18']
itrain_in = itrain_in.drop('time',axis=1)
itrain_in.sort_values(by='stationID', axis=0, ascending=True, inplace=True) 
itrain_in = itrain_in.drop('stationID',axis=1)

itest_in = total_train['2019-01-19':'2019-01-25']
itest_in = itest_in.drop('time',axis=1)
itest_in.sort_values(by='stationID', axis=0, ascending=True, inplace=True) 
itest_in = itest_in.drop('stationID',axis=1)

print(itrain_in.head())
#第二个即为目标字段：userID_in

#构造张量
#print(single_itrain_in)
#print(itrain_in['stationID'].value_counts())
itrain_in = np.array(itrain_in)
itest_in = np.array(itest_in)
#print(itrain_in)
#print(itest_in.shape)

itrain_in = itrain_in.reshape(80, -1, 16)
itest_in = itest_in.reshape(80, -1, 16)
print(itrain_in.shape)
print(itest_in.shape)

#保存训练数据和测试数据
np.save("itrain_in.npy",itrain_in)
np.save("itest_in.npy",itest_in)
```

第二部分：构造STGCN模型<br>
```
class TimeBlock(nn.Module):
    """
    Neural network block that applies a temporal convolution to each node of
    a graph in isolation.
    """
    def __init__(self, in_channels, out_channels, kernel_size=3):
        """
        :param in_channels: Number of input features at each node in each time
        step.
        :param out_channels: Desired number of output channels at each node in
        each time step.
        :param kernel_size: Size of the 1D temporal kernel.
        """

        super(TimeBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))
        self.conv2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))
        self.conv3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))

    def forward(self, X):
        """
        :param X: Input data of shape (batch_size, num_nodes, num_timesteps,
        num_features=in_channels)
        :return: Output data of shape (batch_size, num_nodes,
        num_timesteps_out, num_features_out=out_channels)
        """
        # Convert into NCHW format for pytorch to perform convolutions.
        X = X.permute(0, 3, 1, 2)
        temp = self.conv1(X) + torch.sigmoid(self.conv2(X))
        out = F.relu(temp + self.conv3(X))
        # Convert back from NCHW to NHWC
        out = out.permute(0, 2, 3, 1)

        return out

class STGCNBlock(nn.Module):
    """
    Neural network block that applies a temporal convolution on each node in
    isolation, followed by a graph convolution, followed by another temporal
    convolution on each node.
    """
    def __init__(self, in_channels, spatial_channels, out_channels,
                 num_nodes):
        """
        :param in_channels: Number of input features at each node in each time
        step.
        :param spatial_channels: Number of output channels of the graph
        convolutional, spatial sub-block.
        :param out_channels: Desired number of output features at each node in
        each time step.
        :param num_nodes: Number of nodes in the graph.
        """

        super(STGCNBlock, self).__init__()
        self.temporal1 = TimeBlock(in_channels=in_channels,
                                   out_channels=out_channels)
        self.Theta1 = nn.Parameter(torch.FloatTensor(out_channels,
                                                     spatial_channels))
        self.temporal2 = TimeBlock(in_channels=spatial_channels,
                                   out_channels=out_channels)
        self.batch_norm = nn.BatchNorm2d(num_nodes)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.Theta1.shape[1])
        self.Theta1.data.uniform_(-stdv, stdv)
        
    def forward(self, X, A_hat):
        """
        :param X: Input data of shape (batch_size, num_nodes, num_timesteps,
        num_features=in_channels).
        :param A_hat: Normalized adjacency matrix.
        :return: Output data of shape (batch_size, num_nodes,
        num_timesteps_out, num_features=out_channels).
        """
        #print(X.shape)
        t = self.temporal1(X)
        lfs = torch.einsum("ij,jklm->kilm", [A_hat, t.permute(1, 0, 2, 3)])
        # t2 = F.relu(torch.einsum("ijkl,lp->ijkp", [lfs, self.Theta1]))
        t2 = F.relu(torch.matmul(lfs, self.Theta1))
        t3 = self.temporal2(t2)
        return self.batch_norm(t3)
        # return t3

class STGCN(nn.Module):
    """
    Spatio-temporal graph convolutional network as described in
    https://arxiv.org/abs/1709.04875v3 by Yu et al.
    Input should have shape (batch_size, num_nodes, num_input_time_steps,
    num_features).
    """
    def __init__(self, num_nodes, num_features, num_timesteps_input,
                 num_timesteps_output):
        """
        :param num_nodes: Number of nodes in the graph.
        :param num_features: Number of features at each node in each time step.
        :param num_timesteps_input: Number of past time steps fed into the
        network.
        :param num_timesteps_output: Desired number of future time steps
        output by the network.
        """
        super(STGCN, self).__init__()
        self.block1 = STGCNBlock(in_channels=num_features, out_channels=64,
                                 spatial_channels=16, num_nodes=num_nodes)
        self.block2 = STGCNBlock(in_channels=64, out_channels=64,
                                 spatial_channels=16, num_nodes=num_nodes)
        self.last_temporal = TimeBlock(in_channels=64, out_channels=64)
        self.fully = nn.Linear((num_timesteps_input - 2 * 5) * 64,
                               num_timesteps_output)

    def forward(self, A_hat, X):
        """
        :param X: Input data of shape (batch_size, num_nodes, num_timesteps,
        num_features=in_channels).
        :param A_hat: Normalized adjacency matrix.
        """
        out1 = self.block1(X, A_hat)
        #print(out1.shape)
        out2 = self.block2(out1, A_hat)
        #print("*******************")
        #print(out2.shape)
        out3 = self.last_temporal(out2)
        out4 = self.fully(out3.reshape((out3.shape[0], out3.shape[1], -1)))

        return out4
```


第三部分：构造STGCN模型所需的数据<br>

```
def load_data():
    
    #shape = （节点个数，特征维度，时间总步数）
    itest_in = np.load("itest_in.npy").astype(np.float32).transpose((0, 2, 1))
    itrain_in = np.load("itrain_in.npy").astype(np.float32).transpose((0, 2, 1))
    
    # Normalization using Z-score method
    train_means = np.mean(itrain_in, axis=(0, 2))
    itrain_in = itrain_in - train_means.reshape(1, -1, 1)
    train_stds = np.std(itrain_in, axis=(0, 2))
    itrain_in = itest_in / train_stds.reshape(1, -1, 1)
    
    # Normalization using Z-score method
    test_means = np.mean(itest_in, axis=(0, 2))
    itest_in = itest_in - test_means.reshape(1, -1, 1)
    test_stds = np.std(itest_in, axis=(0, 2))
    itest_in = itest_in / test_stds.reshape(1, -1, 1)
    
    A = pd.read_csv("../data/tianchi/Metro_roadMap.csv")
    
    A.columns = range(len(A.columns))
    A = A.drop(0,axis=1)
    A = A.drop(50,axis=0)
    A = A.drop(50,axis=1)
    A = np.array(A).astype(np.float32)

    return A, itrain_in, itest_in, train_means, train_stds, test_means, test_stds

def get_normalized_adj(A):

    """
    Returns the degree normalized adjacency matrix.
    """
    A = A + np.diag(np.ones(A.shape[0], dtype=np.float32))
    D = np.array(np.sum(A, axis=1)).reshape((-1,))
    D[D <= 10e-5] = 10e-5    # Prevent infs
    diag = np.reciprocal(np.sqrt(D))
    A_wave = np.multiply(np.multiply(diag.reshape((-1, 1)), A),
                         diag.reshape((1, -1)))
    return A_wave


def generate_dataset(X, num_timesteps_input, num_timesteps_output):

    """
    Takes node features for the graph and divides them into multiple samples
    along the time-axis by sliding a window of size (num_timesteps_input+
    num_timesteps_output) across it in steps of 1.
    :param X: Node features of shape (num_vertices, num_features,
    num_timesteps)
    :return:
        - Node features divided into multiple samples. Shape is
          (num_samples, num_vertices, num_features, num_timesteps_input).
        - Node targets for the samples. Shape is
          (num_samples, num_vertices, num_features, num_timesteps_output).
    """

    # Generate the beginning index and the ending index of a sample, which
    # contains (num_points_for_training + num_points_for_predicting) points
    indices = [(i, i + (num_timesteps_input + num_timesteps_output)) for i
               in range(X.shape[2]-(num_timesteps_input + num_timesteps_output) + 1)]

    # Save samples
    features, target = [], []
    for i, j in indices:
        features.append(
            X[:, :, i: i + num_timesteps_input].transpose(
                (0, 2, 1)))
        target.append(X[:, 0, i + num_timesteps_input: j])

    return torch.from_numpy(np.array(features)), torch.from_numpy(np.array(target))
```

第四部分部分：训练并评估模型<br>

```
use_gpu = False
num_timesteps_input = 12
num_timesteps_output = 3

epochs = 100
batch_size = 50

args = {}

if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')


def train_epoch(training_input, training_target, batch_size):

    """
    Trains one epoch with the given data.
    :param training_input: Training inputs of shape (num_samples, num_nodes,
    num_timesteps_train, num_features).
    :param training_target: Training targets of shape (num_samples, num_nodes,
    num_timesteps_predict).
    :param batch_size: Batch size to use during training.
    :return: Average loss for this epoch.
    """

    permutation = torch.randperm(training_input.shape[0])
    #print(permutation)
    epoch_training_losses = []
    for i in range(0, training_input.shape[0], batch_size):
        net.train()
        optimizer.zero_grad()
        indices = permutation[i:i + batch_size]
        X_batch, y_batch = training_input[indices], training_target[indices]
        X_batch = X_batch.to(device=args['device'])
        y_batch = y_batch.to(device=args['device'])
        #print(X_batch.shape)
        out = net(A_wave, X_batch)
        
        loss = loss_criterion(out, y_batch)
        loss.backward()
        optimizer.step()
        
        epoch_training_losses.append(loss.detach().cpu().numpy())
    return sum(epoch_training_losses)/len(epoch_training_losses)


if __name__ == '__main__':
    torch.manual_seed(7)
    A, itrain_in, itest_in, train_means, train_stds, test_means, test_stds = load_data()
    split_line = int(itrain_in.shape[2] * 0.7)
    train_original_data = itrain_in[:, :, :split_line]
    val_original_data = itrain_in[:, :, split_line:]
    test_original_data = itest_in
    training_input, training_target = generate_dataset(train_original_data,
                                                       num_timesteps_input=num_timesteps_input,
                                                       num_timesteps_output=num_timesteps_output)
    val_input, val_target = generate_dataset(val_original_data,
                                             num_timesteps_input=num_timesteps_input,
                                             num_timesteps_output=num_timesteps_output)
    test_input, test_target = generate_dataset(test_original_data,
                                               num_timesteps_input=num_timesteps_input,
                                               num_timesteps_output=num_timesteps_output)
    A_wave = get_normalized_adj(A)
    A_wave = torch.from_numpy(A_wave)
    A_wave = A_wave.to(device=args['device'])
    net = STGCN(A_wave.shape[0],
                training_input.shape[3],
                num_timesteps_input,
                num_timesteps_output).to(device=args['device'])

    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)
    loss_criterion = nn.MSELoss()
    training_losses = []
    validation_losses = []
    validation_maes = []

    for epoch in range(epochs):
        loss = train_epoch(training_input, training_target,
                           batch_size=batch_size)
        print("第%d次迭代损失：%f" % (epoch+1, loss))
        training_losses.append(loss)

        # Run validation
        with torch.no_grad():
            net.eval()
            val_input = val_input.to(device=args['device'])
            val_target = val_target.to(device=args['device'])
            out = net(A_wave, val_input)
            val_loss = loss_criterion(out, val_target).to(device="cpu")
            validation_losses.append(np.asscalar(val_loss.detach().numpy()))
            
            #归一化后的数据还原
            out_unnormalized = out.detach().cpu().numpy()*train_stds[0]+train_means[0]
            target_unnormalized = val_target.detach().cpu().numpy()*train_stds[0]+train_means[0]
            mae = np.mean(np.absolute(out_unnormalized - target_unnormalized))
            validation_maes.append(mae)
            
            out = None
            val_input = val_input.to(device="cpu")
            val_target = val_target.to(device="cpu")

    print("Training loss: {}".format(training_losses[-1]))
    print("Validation loss: {}".format(validation_losses[-1]))
    print("Validation MAE: {}".format(validation_maes[-1]))
    plt.plot(training_losses, label="training loss")
    plt.plot(validation_losses, label="validation loss")
    plt.legend()
    plt.show()
    
    #在测试集上测试效果
    with torch.no_grad():
        test_input = test_input.to(device=args['device'])
        test_target = test_target.to(device=args['device'])
        out = net(A_wave, test_input)

        #归一化后的数据还原
        out_unnormalized = out.detach().cpu().numpy()*test_stds[0]+test_means[0]
        target_unnormalized = test_target.detach().cpu().numpy()*test_stds[0]+test_means[0]
        mae = np.mean(np.absolute(out_unnormalized - target_unnormalized))
        print("Test MAE: {}".format(mae))
```

运行结果：<br>

第94次迭代损失：0.172431<br>
第95次迭代损失：0.183619<br>
第96次迭代损失：0.181750<br>
第97次迭代损失：0.173658<br>
第98次迭代损失：0.175927<br>
第99次迭代损失：0.171645<br>
第100次迭代损失：0.166343<br>
Training loss: 0.1663429599541884<br>
Validation loss: 0.5437355041503906<br>
Validation MAE: 19.35744285583496<br>
Test MAE: 39.35224533081055<br>

<div align=center><img src="https://github.com/xchadesi/Space-time-Sequence/blob/master/stgcn_result.png"/></div>

